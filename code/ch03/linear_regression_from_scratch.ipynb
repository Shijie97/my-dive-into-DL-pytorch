{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from d2lzh_pytorch import *\n",
    "\n",
    "num_inputs = 2\n",
    "num_examples = 10000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.randn(num_examples, num_inputs, dtype = torch.float32).view(-1, 2) # 确保是矩阵\n",
    "labels = features[:, 0] * true_w[0] + features[:, 1] * true_w[1] + true_b # tensor具有广播机制, torch.Size([1000])\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size = labels.size()), dtype = torch.float32)\n",
    "\n",
    "# 一定要保证所有元素是矩阵\n",
    "labels = labels.view(-1, 1) # 这一步很关键，我当时就是找bug这里出现了问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6934,  0.9817],\n",
      "        [-0.2972,  1.8460],\n",
      "        [ 1.1168, -1.1542],\n",
      "        [-0.8056,  2.8604],\n",
      "        [ 0.4212,  0.1680],\n",
      "        [-0.7782,  0.0159],\n",
      "        [-0.2379, -2.1238],\n",
      "        [-1.1629, -2.3002],\n",
      "        [ 0.9462, -1.7461],\n",
      "        [-1.6638, -1.5105]])\n",
      "tensor([[ 4.2500],\n",
      "        [-2.6710],\n",
      "        [10.3481],\n",
      "        [-7.1333],\n",
      "        [ 4.4642],\n",
      "        [ 2.5931],\n",
      "        [10.9495],\n",
      "        [ 9.7045],\n",
      "        [12.0411],\n",
      "        [ 6.0269]])\n"
     ]
    }
   ],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    number_examples = len(features)\n",
    "    \n",
    "    # shuffle\n",
    "    indices = np.random.permutation(number_examples)\n",
    "    \n",
    "    for i in range(0, number_examples, batch_size):\n",
    "        slice = indices[i : num_examples] if (i + batch_size > num_examples) else indices[i : i + batch_size]\n",
    "        yield features[slice], labels[slice]\n",
    "\n",
    "# test\n",
    "for x, y in data_iter(batch_size = 10, features = features, labels = labels):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = torch.tensor(np.random.normal(0, 0.01, size = (num_inputs, 1)), dtype = torch.float32, requires_grad = True)\n",
    "b = torch.tensor(1, dtype = torch.float32, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(x, w, b):\n",
    "    return torch.matmul(x, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回的可能是标量，也可能是向量\n",
    "def squared_loss(y, y_t):\n",
    "    return (y - y_t) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为这里求得的梯度是所有batch的梯度之和，所以要除以一个batch大小\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "#         print(param.grad)\n",
    "        param.data -= (lr * param.grad / batch_size) # 梯度更新时w和b不参与反向传播，要用.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, loss = 5.0155489589087665e-05\n",
      "epoch = 1, loss = 5.0383026973577216e-05\n",
      "epoch = 2, loss = 5.023623089073226e-05\n",
      "epoch = 3, loss = 5.009843880543485e-05\n",
      "epoch = 4, loss = 5.016545037506148e-05\n"
     ]
    }
   ],
   "source": [
    "epoch_number = 5\n",
    "lr = 0.1\n",
    "batchs_size = 50\n",
    "\n",
    "for i in range(epoch_number):\n",
    "    for data, label in data_iter(batchs_size, features, labels):\n",
    "        model_res = linreg(data, w, b)\n",
    "        loss = squared_loss(label, model_res).sum()\n",
    "        loss.backward()\n",
    "        sgd([w, b], lr, batchs_size)\n",
    "        \n",
    "        # 记得清零\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "\n",
    "    \n",
    "    # 计算当前epoch的loss\n",
    "    loss_current_epoch = squared_loss(labels, linreg(features, w, b)).mean().item()\n",
    "    print('epoch = ' + str(i) + ', loss = ' + str(loss_current_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4]\n",
      "tensor([[ 2.0005],\n",
      "        [-3.3998]], requires_grad=True)\n",
      "**************************************************\n",
      "4.2\n",
      "tensor(4.1999, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(true_w)\n",
    "print(w)\n",
    "print('*' * 50)\n",
    "print(true_b)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
